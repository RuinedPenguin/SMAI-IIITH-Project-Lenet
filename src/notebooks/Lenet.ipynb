{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49d8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os , sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cd4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local imports, comment in colab if required\n",
    "root = os.getcwd()\n",
    "sys.path.append(root[: root.rindex('src')+4]+'scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2255c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import zero_pad\n",
    "from lenet import Lenet_SMAI\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "import struct\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdd155-99cc-4759-80a7-506f5f326da8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03454dc-de7c-434a-875b-dbed3dea9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images and labels\n",
    "def readDataset(dataset):\n",
    "    (image, label) = dataset\n",
    "    with open(label, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(image, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    return (img, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa6101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of the dataset\n",
    "test_image_path = r'../../MNIST/t10k-images-idx3-ubyte'\n",
    "test_label_path = r'../../MNIST/t10k-labels-idx1-ubyte'\n",
    "train_image_path = r'../../MNIST/train-images-idx3-ubyte'\n",
    "train_label_path = r'../../MNIST/train-labels-idx1-ubyte'\n",
    "training_set = (train_image_path, train_label_path)\n",
    "test_set = (test_image_path, test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88f9f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training images dataset: (60000, 28, 28)\n",
      "The shape of testing images dataset:  (10000, 28, 28)\n",
      "Length of the training set:  60000\n",
      "Length of the test set:  10000\n",
      "Shape of a single image:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "# read the dataset with readDataset()\n",
    "(train_images, train_labels) = readDataset(training_set)\n",
    "(test_images, test_labels) = readDataset(test_set)\n",
    "n_m, n_m_test = len(train_labels), len(test_labels)\n",
    "print(\"The shape of training images dataset:\", train_images.shape)\n",
    "print(\"The shape of testing images dataset: \", test_images.shape)\n",
    "print(\"Length of the training set: \", n_m)\n",
    "print(\"Length of the test set: \", n_m_test)\n",
    "print(\"Shape of a single image: \", train_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f340e",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbd8953-9cbb-4813-a884-cc7656807bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of the input images\n",
    "def normalize(image):\n",
    "    image -= image.min()\n",
    "    image = image / image.max()\n",
    "    # range = [-0.1,1.175]   \n",
    "    image = image * 1.275 - 0.1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c010d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training image with padding: (60000, 32, 32, 1)\n",
      "The shape of testing image with padding:  (10000, 32, 32, 1)\n",
      "num of train labels (60000,)\n",
      "num of test labels (10000,)\n"
     ]
    }
   ],
   "source": [
    "X = normalize(zero_pad(train_images[:,:,:,np.newaxis], 2))\n",
    "X_test  = normalize(zero_pad(test_images[:,:,:,np.newaxis],  2))\n",
    "y = train_labels\n",
    "y_test = test_labels\n",
    "print(\"The shape of training image with padding:\", X.shape)\n",
    "print(\"The shape of testing image with padding: \", X_test.shape)\n",
    "print(\"num of train labels\", y.shape)\n",
    "print(\"num of test labels\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fac6cbe-8152-470d-9f23-027b4f4900c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d76e5de10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d8580",
   "metadata": {},
   "source": [
    "### Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3436f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, ratio = 0.1):\n",
    "    indices = list(np.random.choice(X.shape[0] , int(X.shape[0] * ratio), replace=False))\n",
    "    left_over = list(set(range(X.shape[0])) - set(indices))\n",
    "    return X[left_over, :, :, :], X[indices, : , :, :], y[left_over], y[indices]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102b9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1) (6000, 32, 32, 1) (54000,) (6000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, 0.1)\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6a9b5-8026-4a03-91cb-92afa93bb6fb",
   "metadata": {},
   "source": [
    "## Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963faec-9868-4921-905b-928b1691514b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e03c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-------Lenet-------\t\t\n",
      "+--------------+-----------------------------------------+--------------+----------------+\n",
      "| in           | Name (weight, bias)                     | out          |   total_params |\n",
      "|--------------+-----------------------------------------+--------------+----------------|\n",
      "| (32, 32, 1)  | CONV2D((5, 5, 1, 6), (1, 1, 1, 6))      | (28, 28, 6)  |            156 |\n",
      "| (28, 28, 6)  | Activation(relu)                        | (28, 28, 6)  |              0 |\n",
      "| (28, 28, 6)  | SubSample((1, 1, 1, 6), (1, 1, 1, 6))   | (14, 14, 6)  |             12 |\n",
      "| (14, 14, 6)  | CONV2D((5, 5, 6, 16), (1, 1, 1, 16))    | (10, 10, 16) |           2416 |\n",
      "| (10, 10, 16) | Activation(relu)                        | (10, 10, 16) |              0 |\n",
      "| (10, 10, 16) | SubSample((1, 1, 1, 16), (1, 1, 1, 16)) | (5, 5, 16)   |             32 |\n",
      "| (5, 5, 16)   | CONV2D((5, 5, 16, 120), (1, 1, 1, 120)) | (1, 1, 120)  |          48120 |\n",
      "| (1, 1, 120)  | Activation(tanh)                        | (1, 1, 120)  |              0 |\n",
      "| (1, 1, 120)  | Dense((120, 84), (84,))                 | (84, 1)      |          10164 |\n",
      "| (84, 1)      | Activation(tanh)                        | (84, 1)      |              0 |\n",
      "| (84, 1)      | RBF(10, 84)                             | (1, 1)       |            840 |\n",
      "+--------------+-----------------------------------------+--------------+----------------+\n",
      "Total Number of parameters = 61740\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = Lenet_SMAI()\n",
    "model.summary()\n",
    "model.compile_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94a2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# itr = 500\n",
    "# st = time.time()\n",
    "# img, label = np.random.rand(batch_size, 32, 32, 1), np.random.randint(0, 10,(batch_size,))\n",
    "# for i in range(itr):\n",
    "#     loss = model(img, label)\n",
    "#     print(loss)\n",
    "#     grads = model.compute_gradients()\n",
    "#     model.apply_gradients(grads)\n",
    "# print(f\"accuracy : {np.count_nonzero(model(img, mode='test') == label) / batch_size}\")\n",
    "# print(f'took {time.time() - st} for {itr} batch steps of size {batch_size}, 1 prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9733deb",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6df9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "from matplotlib.pyplot import figure\n",
    "import time\n",
    "class PeriodicPlotter:\n",
    "  def __init__(self, sec, xlabel='', ylabel='', scale=None, figsize = (8,6)):\n",
    "\n",
    "    self.xlabel = xlabel\n",
    "    self.ylabel = ylabel\n",
    "    self.sec = sec\n",
    "    self.scale = scale\n",
    "    self.figsize = figsize\n",
    "\n",
    "    self.tic = time.time()\n",
    "\n",
    "  def plot(self, data, epoch):\n",
    "    if time.time() - self.tic > self.sec:\n",
    "      plt.cla()\n",
    "      plt.plot(data[0])\n",
    "      if len(epoch) > 0: plt.vlines(x = epoch, ymin = 0, ymax = max(data[0]))\n",
    "      if len(data[1]) > 0:\n",
    "          plt.plot(data[1])\n",
    "          plt.legend([\"train_loss\", \"val_loss\"])\n",
    "      else:\n",
    "        plt.legend([\"train_loss\"])\n",
    "      plt.ylabel(self.ylabel)\n",
    "      plt.xlabel(self.xlabel)\n",
    "      plt.tight_layout()\n",
    "      ipythondisplay.clear_output(wait=True)\n",
    "      ipythondisplay.display(plt.gcf())\n",
    "\n",
    "      self.tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322ffc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = PeriodicPlotter(xlabel='iterations', ylabel='loss', sec = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766e42a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e929df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 28\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "730346bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_val_loss(lenet_model, x_val, y_val):\n",
    "    batch_size = 512\n",
    "    return np.mean([lenet_model(x_val[i:i+batch_size, :, :, :], y_val[i:i+batch_size]) for i in range(0, x_val.shape[0], batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b1342b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(lenet_model, x, y):\n",
    "    batch_size = 512\n",
    "    acc = []\n",
    "    for i in tqdm(range(0, x.shape[0], batch_size)):\n",
    "        pred = lenet_model(x[i:i+batch_size, :, :, :], None, 'test')\n",
    "        acc.append(np.count_nonzero(y[i:i+batch_size] == pred) / pred.shape[0])\n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecc6d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, x):\n",
    "    batch_size = 512\n",
    "    res = []\n",
    "    for i in tqdm(range(0, x.shape[0], batch_size)):\n",
    "        pred = model(x[i:i+batch_size, :, :, :], None, 'test')\n",
    "        res+=list(pred)\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19de3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model and plotting loss dynamically\n",
    "for e in range(epochs):\n",
    "    loss = []\n",
    "    k = 0\n",
    "    t = tqdm(range(0, x_train.shape[0] ,batch_size))\n",
    "    for i in t:\n",
    "        input = x_train[i : i + batch_size , :, :, :]\n",
    "        labels = y_train[i : i + batch_size]\n",
    "        loss.append(model(input, labels))\n",
    "        grads = model.compute_gradients()\n",
    "        model.apply_gradients(grads)\n",
    "        k+=1\n",
    "        if k % 20 == 0:\n",
    "            history[0].append(np.mean(loss))\n",
    "            history[1].append(mean_val_loss(model, x_val, y_val))\n",
    "            val_acc = accuracy(model, x_val, y_val)\n",
    "            t.set_description(f'epoch = {e + 1}, train_loss = {history[0][-1]}, val_loss = {history[1][-1]}, val_acc = {val_acc}')\n",
    "            loss = []\n",
    "            plotter.plot(history, [])\n",
    "    history[0].append(np.mean(loss))\n",
    "    history[1].append(mean_val_loss(model, x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a26714b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  6.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9907571231617647"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the model\n",
    "accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff702d",
   "metadata": {},
   "source": [
    "#### Save and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aace1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e244408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model):\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n",
    "        weights.append(layer.params)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3523f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights):\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        print(layer) \n",
    "        layer.params = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b718451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV2D((5, 5, 1, 6), (1, 1, 1, 6))\n",
      "Activation(relu)\n",
      "SubSample((1, 1, 1, 6), (1, 1, 1, 6))\n",
      "CONV2D((5, 5, 6, 16), (1, 1, 1, 16))\n",
      "Activation(relu)\n",
      "SubSample((1, 1, 1, 16), (1, 1, 1, 16))\n",
      "CONV2D((5, 5, 16, 120), (1, 1, 1, 120))\n",
      "Activation(tanh)\n",
      "Dense((120, 84), (84,))\n",
      "Activation(tanh)\n",
      "RBF(10, 84)\n"
     ]
    }
   ],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pk.dump(model_save(model), f, pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfb1d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV2D((5, 5, 1, 6), (1, 1, 1, 6))\n",
      "Activation(relu)\n",
      "SubSample((1, 1, 1, 6), (1, 1, 1, 6))\n",
      "CONV2D((5, 5, 6, 16), (1, 1, 1, 16))\n",
      "Activation(relu)\n",
      "SubSample((1, 1, 1, 16), (1, 1, 1, 16))\n",
      "CONV2D((5, 5, 16, 120), (1, 1, 1, 120))\n",
      "Activation(tanh)\n",
      "Dense((120, 84), (84,))\n",
      "Activation(tanh)\n",
      "RBF(10, 84)\n"
     ]
    }
   ],
   "source": [
    "model_2 = Lenet_SMAI()\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    weights = pk.load(f)\n",
    "    load_weights(model_2, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae7a56dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9907571231617647"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model_2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737f465-2644-4eb6-90e0-3b080e9f8abf",
   "metadata": {},
   "source": [
    "#### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d847130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  6.69it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = predictions(model_2, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92ca33a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
